{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0883d102",
   "metadata": {},
   "source": [
    "# Chu·∫©n h√≥a d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu v√† ph√¢n c√¥ng gi·∫£ng vi√™n\n",
    "\n",
    "Notebook n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:\n",
    "1. Chu·∫©n h√≥a d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu t·ª´ file CSV\n",
    "2. Ph√¢n c√¥ng gi·∫£ng vi√™n ph√π h·ª£p d·ª±a tr√™n khoa/b·ªô m√¥n\n",
    "3. ƒê·∫£m b·∫£o kh√¥ng c√≥ xung ƒë·ªôt v·ªÅ th·ªùi gian gi·∫£ng d·∫°y\n",
    "4. Lo·∫°i b·ªè c√°c m√¥n h·ªçc kh√¥ng c·∫ßn ph√¢n c√¥ng gi·∫£ng vi√™n (DA, TTTN, KLTN, PE231)\n",
    "\n",
    "## Quy t·∫Øc ph√¢n c√¥ng:\n",
    "- M√¥n h·ªçc thu·ªôc khoa/b·ªô m√¥n n√†o th√¨ gi·∫£ng vi√™n khoa ·∫•y d·∫°y\n",
    "- Gi·∫£ng vi√™n kh√¥ng th·ªÉ d·∫°y c√°c l·ªõp kh√°c nhau trong c√πng m·ªôt ti·∫øt h·ªçc\n",
    "- Ph√≤ng h·ªçc/l·ªãch h·ªçc c√≥ d·∫•u \"*\" ƒë∆∞·ª£c gi·ªØ nguy√™n (online/kh√¥ng ph√¢n chia)\n",
    "- Lo·∫°i tr·ª´: DA, TTTN, KLTN, PE231\n",
    "- **C√°c m√¥n b·∫Øt ƒë·∫ßu b·∫±ng NT (nh∆∞ NT101, NT102...) thu·ªôc khoa MMTTT v√† ƒë∆∞·ª£c ph√¢n c√¥ng gi·∫£ng vi√™n b√¨nh th∆∞·ªùng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efc5324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14062972",
   "metadata": {},
   "source": [
    "## 1. Load v√† Explore Data\n",
    "\n",
    "T·∫£i d·ªØ li·ªáu t·ª´ c√°c file CSV v√† kh√°m ph√° c·∫•u tr√∫c d·ªØ li·ªáu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ac61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i d·ªØ li·ªáu...\n",
      "ƒê√£ t·∫£i 824 d√≤ng d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu\n",
      "ƒê√£ t·∫£i 242 gi·∫£ng vi√™n\n",
      "ƒê√£ t·∫£i 961 m√¥n h·ªçc\n",
      "\n",
      "T·∫£i d·ªØ li·ªáu ho√†n t·∫•t!\n"
     ]
    }
   ],
   "source": [
    "# Load d·ªØ li·ªáu t·ª´ c√°c file CSV\n",
    "print(\"ƒêang t·∫£i d·ªØ li·ªáu...\")\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c file d·ªØ li·ªáu\n",
    "schedule_path = r\"d:\\eUIT\\scripts\\database\\other_data\\thoi_khoa_bieu.csv\"\n",
    "teachers_path = r\"d:\\eUIT\\scripts\\database\\main_data\\giang_vien.csv\"\n",
    "subjects_path = r\"d:\\eUIT\\scripts\\database\\main_data\\danh_muc_mon_hoc.csv\"\n",
    "\n",
    "# Load d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu\n",
    "df_schedule = pd.read_csv(schedule_path, sep=';', encoding='utf-8')\n",
    "print(f\"ƒê√£ t·∫£i {len(df_schedule)} d√≤ng d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu\")\n",
    "\n",
    "# Load d·ªØ li·ªáu gi·∫£ng vi√™n\n",
    "df_teachers = pd.read_csv(teachers_path, encoding='utf-8')\n",
    "print(f\"ƒê√£ t·∫£i {len(df_teachers)} gi·∫£ng vi√™n\")\n",
    "\n",
    "# Load d·ªØ li·ªáu m√¥n h·ªçc\n",
    "df_subjects = pd.read_csv(subjects_path, sep=';', encoding='utf-8')\n",
    "print(f\"ƒê√£ t·∫£i {len(df_subjects)} m√¥n h·ªçc\")\n",
    "\n",
    "print(\"\\nT·∫£i d·ªØ li·ªáu ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9236e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== C·∫§U TR√öC D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\n",
      "Columns: ['hoc_ky', 'ma_mon_hoc', 'ma_lop', 'so_tin_chi', 'ma_giang_vien', 'thu', 'tiet', 'cach_tuan', 'ngay_bat_dau', 'ngay_ket_thuc', 'phong_hoc', 'si_so', 'hinh_thuc_giang_day', 'ghi_chu']\n",
      "Shape: (824, 14)\n",
      "\n",
      "Sample data:\n",
      "        hoc_ky ma_mon_hoc      ma_lop  so_tin_chi  ma_giang_vien thu  \\\n",
      "0  2024_2025_2     BCH058  BCH058.P21           2            NaN   4   \n",
      "1  2024_2025_2     BCH058  BCH058.P22           2            NaN   4   \n",
      "2  2024_2025_2      ENG03   ENG03.P21           4            NaN   3   \n",
      "3  2024_2025_2      ENG03   ENG03.P21           4            NaN   5   \n",
      "4  2024_2025_2      ENG01   ENG01.P21           4            NaN   3   \n",
      "\n",
      "       tiet  cach_tuan ngay_bat_dau ngay_ket_thuc phong_hoc  si_so  \\\n",
      "0     12345          1   03/03/2025    10/05/2025       NaN    NaN   \n",
      "1      6789          1   03/03/2025    24/05/2025         *    NaN   \n",
      "2  11,12,13          1   17/02/2025    17/05/2025         *    NaN   \n",
      "3  11,12,13          1   17/02/2025    17/05/2025         *    NaN   \n",
      "4  11,12,13          1   17/02/2025    17/05/2025         *    NaN   \n",
      "\n",
      "  hinh_thuc_giang_day                   ghi_chu  \n",
      "0                  LT  H·ªçc t·∫°i tr∆∞·ªùng ƒêHKHXH&NV  \n",
      "1                  LT  H·ªçc t·∫°i tr∆∞·ªùng ƒêHKHXH&NV  \n",
      "2                  LT                       NaN  \n",
      "3                  LT                       NaN  \n",
      "4                  LT                       NaN  \n",
      "\n",
      "=== C·∫§U TR√öC D·ªÆ LI·ªÜU GI·∫¢NG VI√äN ===\n",
      "Columns: ['ma_giang_vien', 'ho_ten', 'khoa_bo_mon', 'ngay_sinh', 'noi_sinh', 'cccd', 'ngay_cap_cccd', 'noi_cap_cccd', 'dan_toc', 'ton_giao', 'so_dien_thoai', 'dia_chi_thuong_tru', 'tinh_thanh_pho', 'phuong_xa']\n",
      "Shape: (242, 14)\n",
      "\n",
      "Khoa/B·ªô m√¥n c·ªßa gi·∫£ng vi√™n:\n",
      "khoa_bo_mon\n",
      "HTTT     55\n",
      "KHMT     45\n",
      "KTMT     35\n",
      "MMTTT    32\n",
      "KTTT     27\n",
      "CNPM     23\n",
      "PƒêTƒêH    13\n",
      "BMTL      6\n",
      "TTNN      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== C·∫§U TR√öC D·ªÆ LI·ªÜU M√îN H·ªåC ===\n",
      "Columns: ['M√£ MH', 'T√™n MH (Ti·∫øng Vi·ªát)', 'T√™n MH (Ti·∫øng Anh)', 'C√≤n m·ªü l·ªõp', 'ƒê∆°n v·ªã qu·∫£n l√Ω chuy√™n m√¥n', 'Lo·∫°i MH', 'S·ªë TCLT', 'S·ªë TCTH']\n",
      "Shape: (961, 8)\n",
      "\n",
      "B·ªô m√¥n qu·∫£n l√Ω m√¥n h·ªçc:\n",
      "ƒê∆°n v·ªã qu·∫£n l√Ω chuy√™n m√¥n\n",
      "HTTT     210\n",
      "KHMT     171\n",
      "KTMT     130\n",
      "MMTTT    121\n",
      "KTTT     106\n",
      "CNPM      93\n",
      "PƒêTƒêH     56\n",
      "BMTL      28\n",
      "TTNN      26\n",
      "BMAV       9\n",
      "MMT        7\n",
      "GDQP       1\n",
      "GDTC       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Kh√°m ph√° c·∫•u tr√∫c d·ªØ li·ªáu\n",
    "print(\"=== C·∫§U TR√öC D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\")\n",
    "print(\"Columns:\", list(df_schedule.columns))\n",
    "print(\"Shape:\", df_schedule.shape)\n",
    "print(\"\\nSample data:\")\n",
    "print(df_schedule.head())\n",
    "\n",
    "print(\"\\n=== C·∫§U TR√öC D·ªÆ LI·ªÜU GI·∫¢NG VI√äN ===\")\n",
    "print(\"Columns:\", list(df_teachers.columns))\n",
    "print(\"Shape:\", df_teachers.shape)\n",
    "print(\"\\nKhoa/B·ªô m√¥n c·ªßa gi·∫£ng vi√™n:\")\n",
    "print(df_teachers['khoa_bo_mon'].value_counts())\n",
    "\n",
    "print(\"\\n=== C·∫§U TR√öC D·ªÆ LI·ªÜU M√îN H·ªåC ===\")\n",
    "print(\"Columns:\", list(df_subjects.columns))\n",
    "print(\"Shape:\", df_subjects.shape)\n",
    "print(\"\\nB·ªô m√¥n qu·∫£n l√Ω m√¥n h·ªçc:\")\n",
    "print(df_subjects['ƒê∆°n v·ªã qu·∫£n l√Ω chuy√™n m√¥n'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b6cda",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning v√† Preprocessing\n",
    "\n",
    "L√†m s·∫°ch d·ªØ li·ªáu v√† chu·∫©n b·ªã cho qu√° tr√¨nh chu·∫©n h√≥a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb2b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\n",
      "Missing values:\n",
      "hoc_ky                   0\n",
      "ma_mon_hoc               0\n",
      "ma_lop                   0\n",
      "so_tin_chi               0\n",
      "ma_giang_vien          824\n",
      "thu                      0\n",
      "tiet                     0\n",
      "cach_tuan                0\n",
      "ngay_bat_dau             0\n",
      "ngay_ket_thuc            0\n",
      "phong_hoc                1\n",
      "si_so                  824\n",
      "hinh_thuc_giang_day      0\n",
      "ghi_chu                715\n",
      "dtype: int64\n",
      "ƒê√£ x·ª≠ l√Ω 824 b·∫£n ghi th·ªùi kh√≥a bi·ªÉu\n",
      "Ho√†n t·∫•t l√†m s·∫°ch d·ªØ li·ªáu!\n"
     ]
    }
   ],
   "source": [
    "# L√†m s·∫°ch d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu\n",
    "print(\"=== CLEANING D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\")\n",
    "\n",
    "# T·∫°o b·∫£n copy ƒë·ªÉ x·ª≠ l√Ω\n",
    "df_schedule_clean = df_schedule.copy()\n",
    "\n",
    "# Ki·ªÉm tra v√† x·ª≠ l√Ω missing values\n",
    "print(\"Missing values:\")\n",
    "print(df_schedule_clean.isnull().sum())\n",
    "\n",
    "# X·ª≠ l√Ω c√°c c·ªôt ng√†y th√°ng\n",
    "df_schedule_clean['ngay_bat_dau'] = pd.to_datetime(df_schedule_clean['ngay_bat_dau'], format='%d/%m/%Y')\n",
    "df_schedule_clean['ngay_ket_thuc'] = pd.to_datetime(df_schedule_clean['ngay_ket_thuc'], format='%d/%m/%Y')\n",
    "\n",
    "# X·ª≠ l√Ω c·ªôt ti·∫øt h·ªçc - chuy·ªÉn th√†nh tiet_bat_dau v√† tiet_ket_thuc\n",
    "def parse_time_slots(tiet_str):\n",
    "    \"\"\"Parse time slots like '123', '1234', '6789' etc.\"\"\"\n",
    "    if pd.isna(tiet_str) or str(tiet_str) == '*':\n",
    "        return None, None\n",
    "    \n",
    "    tiet_str = str(tiet_str).replace(',', '').replace(' ', '')\n",
    "    if tiet_str.isdigit():\n",
    "        digits = [int(d) for d in tiet_str]\n",
    "        return min(digits), max(digits)\n",
    "    return None, None\n",
    "\n",
    "df_schedule_clean[['tiet_bat_dau', 'tiet_ket_thuc']] = df_schedule_clean['tiet'].apply(\n",
    "    lambda x: pd.Series(parse_time_slots(x))\n",
    ")\n",
    "\n",
    "print(f\"ƒê√£ x·ª≠ l√Ω {len(df_schedule_clean)} b·∫£n ghi th·ªùi kh√≥a bi·ªÉu\")\n",
    "\n",
    "# Chu·∫©n h√≥a t√™n c·ªôt c·ªßa b·∫£ng m√¥n h·ªçc\n",
    "df_subjects_clean = df_subjects.copy()\n",
    "df_subjects_clean.columns = ['ma_mon_hoc', 'ten_mon_hoc_vi', 'ten_mon_hoc_en', 'con_mo_lop', \n",
    "                            'khoa_bo_mon_quan_ly', 'loai_mon_hoc', 'so_tc_ly_thuyet', 'so_tc_thuc_hanh']\n",
    "\n",
    "print(\"Ho√†n t·∫•t l√†m s·∫°ch d·ªØ li·ªáu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730a8c4",
   "metadata": {},
   "source": [
    "## 3. Normalize Schedule Data\n",
    "\n",
    "Chu·∫©n h√≥a d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu v√† x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be7a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHU·∫®N H√ìA D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\n",
      "S·ªë l·ªõp b·ªã lo·∫°i tr·ª´ kh·ªèi ph√¢n c√¥ng: 123\n",
      "S·ªë l·ªõp c·∫ßn ph√¢n c√¥ng gi·∫£ng vi√™n: 701\n",
      "\n",
      "Ph√¢n t√≠ch l√Ω do lo·∫°i tr·ª´:\n",
      "- H√¨nh th·ª©c DA/TTTN/KLTN: 32\n",
      "- M√¥n PE231: 9\n",
      "- Thu/Ti·∫øt c√≥ d·∫•u '*': 114\n",
      "\n",
      "S·ªë m√¥n NT (thu·ªôc khoa MMTTT): 124\n",
      "C√°c m√¥n NT s·∫Ω ƒë∆∞·ª£c ph√¢n c√¥ng gi·∫£ng vi√™n khoa MMTTT\n",
      "\n",
      "S·ªë l·ªõp c√≥ ph√≤ng h·ªçc '*' (online/kh√¥ng ph√¢n chia): 134\n",
      "Ho√†n t·∫•t chu·∫©n h√≥a d·ªØ li·ªáu!\n"
     ]
    }
   ],
   "source": [
    "# Chu·∫©n h√≥a d·ªØ li·ªáu v√† x√°c ƒë·ªãnh c√°c m√¥n h·ªçc kh√¥ng c·∫ßn ph√¢n c√¥ng gi·∫£ng vi√™n\n",
    "print(\"=== CHU·∫®N H√ìA D·ªÆ LI·ªÜU TH·ªúI KH√ìA BI·ªÇU ===\")\n",
    "\n",
    "df_normalized = df_schedule_clean.copy()\n",
    "\n",
    "# X√°c ƒë·ªãnh c√°c m√¥n h·ªçc kh√¥ng c·∫ßn ph√¢n c√¥ng gi·∫£ng vi√™n\n",
    "exclusion_conditions = [\n",
    "    # M√¥n c√≥ h√¨nh th·ª©c DA, TTTN, KLTN\n",
    "    df_normalized['hinh_thuc_giang_day'].isin(['DA', 'ƒêA', 'TTTN', 'KLTN']),\n",
    "    \n",
    "    # M√¥n PE231\n",
    "    df_normalized['ma_mon_hoc'] == 'PE231',\n",
    "    \n",
    "    # C√°c m√¥n c√≥ thu ho·∫∑c tiet l√† '*'\n",
    "    (df_normalized['thu'] == '*') | (df_normalized['tiet'] == '*')\n",
    "]\n",
    "\n",
    "# T·∫°o c·ªôt ƒë√°nh d·∫•u c√°c m√¥n lo·∫°i tr·ª´\n",
    "df_normalized['loai_tru'] = False\n",
    "for condition in exclusion_conditions:\n",
    "    df_normalized['loai_tru'] = df_normalized['loai_tru'] | condition\n",
    "\n",
    "# Th·ªëng k√™ c√°c m√¥n b·ªã lo·∫°i tr·ª´\n",
    "excluded_count = df_normalized['loai_tru'].sum()\n",
    "print(f\"S·ªë l·ªõp b·ªã lo·∫°i tr·ª´ kh·ªèi ph√¢n c√¥ng: {excluded_count}\")\n",
    "print(f\"S·ªë l·ªõp c·∫ßn ph√¢n c√¥ng gi·∫£ng vi√™n: {len(df_normalized) - excluded_count}\")\n",
    "\n",
    "print(\"\\nPh√¢n t√≠ch l√Ω do lo·∫°i tr·ª´:\")\n",
    "print(\"- H√¨nh th·ª©c DA/TTTN/KLTN:\", \n",
    "      df_normalized['hinh_thuc_giang_day'].isin(['DA', 'ƒêA', 'TTTN', 'KLTN']).sum())\n",
    "print(\"- M√¥n PE231:\", \n",
    "      (df_normalized['ma_mon_hoc'] == 'PE231').sum())\n",
    "print(\"- Thu/Ti·∫øt c√≥ d·∫•u '*':\", \n",
    "      ((df_normalized['thu'] == '*') | (df_normalized['tiet'] == '*')).sum())\n",
    "\n",
    "# Ki·ªÉm tra c√°c m√¥n NT (thu·ªôc khoa MMTTT)\n",
    "nt_courses = df_normalized[df_normalized['ma_mon_hoc'].str.startswith('NT', na=False)]\n",
    "print(f\"\\nS·ªë m√¥n NT (thu·ªôc khoa MMTTT): {len(nt_courses)}\")\n",
    "print(\"C√°c m√¥n NT s·∫Ω ƒë∆∞·ª£c ph√¢n c√¥ng gi·∫£ng vi√™n khoa MMTTT\")\n",
    "\n",
    "# X·ª≠ l√Ω ph√≤ng h·ªçc c√≥ d·∫•u '*' (gi·ªØ nguy√™n)\n",
    "print(f\"\\nS·ªë l·ªõp c√≥ ph√≤ng h·ªçc '*' (online/kh√¥ng ph√¢n chia): {(df_normalized['phong_hoc'] == '*').sum()}\")\n",
    "\n",
    "print(\"Ho√†n t·∫•t chu·∫©n h√≥a d·ªØ li·ªáu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d6b68",
   "metadata": {},
   "source": [
    "## 4. Assign Teachers to Classes\n",
    "\n",
    "Ph√¢n c√¥ng gi·∫£ng vi√™n d·ª±a tr√™n khoa/b·ªô m√¥n v√† tr√°nh xung ƒë·ªôt th·ªùi gian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e7da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== T·∫†O MAPPING M√îN H·ªåC - KHOA/B·ªò M√îN ===\n",
      "ƒê√£ g√°n 124 m√¥n NT cho khoa MMTTT\n",
      "S·ªë l·ªõp kh√¥ng t√¨m th·∫•y th√¥ng tin khoa/b·ªô m√¥n: 0\n",
      "Ph√¢n b·ªë khoa/b·ªô m√¥n c·ªßa c√°c l·ªõp h·ªçc:\n",
      "khoa_bo_mon_quan_ly\n",
      "MMTTT    139\n",
      "KHMT     128\n",
      "KTMT     123\n",
      "CNPM     117\n",
      "HTTT      92\n",
      "KTTT      79\n",
      "PƒêTƒêH     70\n",
      "BMTL      50\n",
      "TTNN      26\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o mapping gi·ªØa m√¥n h·ªçc v√† khoa/b·ªô m√¥n qu·∫£n l√Ω\n",
    "print(\"=== T·∫†O MAPPING M√îN H·ªåC - KHOA/B·ªò M√îN ===\")\n",
    "\n",
    "# Merge ƒë·ªÉ l·∫•y th√¥ng tin khoa/b·ªô m√¥n c·ªßa m√¥n h·ªçc\n",
    "df_with_dept = df_normalized.merge(\n",
    "    df_subjects_clean[['ma_mon_hoc', 'khoa_bo_mon_quan_ly']], \n",
    "    on='ma_mon_hoc', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√°c m√¥n NT (thu·ªôc khoa MMTTT)\n",
    "nt_mask = df_with_dept['ma_mon_hoc'].str.startswith('NT', na=False)\n",
    "df_with_dept.loc[nt_mask, 'khoa_bo_mon_quan_ly'] = 'MMTTT'\n",
    "print(f\"ƒê√£ g√°n {nt_mask.sum()} m√¥n NT cho khoa MMTTT\")\n",
    "\n",
    "# Ki·ªÉm tra m√¥n h·ªçc kh√¥ng c√≥ th√¥ng tin khoa/b·ªô m√¥n\n",
    "missing_subjects = df_with_dept[df_with_dept['khoa_bo_mon_quan_ly'].isna()]\n",
    "print(f\"S·ªë l·ªõp kh√¥ng t√¨m th·∫•y th√¥ng tin khoa/b·ªô m√¥n: {len(missing_subjects)}\")\n",
    "\n",
    "if len(missing_subjects) > 0:\n",
    "    print(\"C√°c m√¥n h·ªçc kh√¥ng c√≥ th√¥ng tin:\")\n",
    "    print(missing_subjects[['ma_mon_hoc', 'ma_lop']].drop_duplicates())\n",
    "\n",
    "# Mapping ƒë·ªÉ chu·∫©n h√≥a t√™n khoa/b·ªô m√¥n\n",
    "dept_mapping = {\n",
    "    'HTTT': 'HTTT',\n",
    "    'KHMT': 'KHMT', \n",
    "    'KTMT': 'KTMT',\n",
    "    'KTTT': 'KTTT',\n",
    "    'MMTTT': 'MMTTT',\n",
    "    'PƒêTƒêH': 'PƒêTƒêH',\n",
    "    'TTNN': 'TTNN',\n",
    "    'BMTL': 'BMTL'\n",
    "}\n",
    "\n",
    "# √Åp d·ª•ng mapping\n",
    "df_with_dept['khoa_bo_mon_quan_ly'] = df_with_dept['khoa_bo_mon_quan_ly'].map(dept_mapping).fillna(df_with_dept['khoa_bo_mon_quan_ly'])\n",
    "\n",
    "print(\"Ph√¢n b·ªë khoa/b·ªô m√¥n c·ªßa c√°c l·ªõp h·ªçc:\")\n",
    "print(df_with_dept['khoa_bo_mon_quan_ly'].value_counts())\n",
    "\n",
    "# C·∫≠p nh·∫≠t df_final\n",
    "df_final = df_with_dept.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8de13b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PH√ÇN C√îNG GI·∫¢NG VI√äN TH√îNG MINH V2 ===\n",
      "B·∫Øt ƒë·∫ßu ph√¢n c√¥ng theo m·ª©c ƒë·ªô ∆∞u ti√™n...\n",
      "\n",
      "K·∫øt qu·∫£ ph√¢n c√¥ng th√¥ng minh V2:\n",
      "- ƒê√£ ph√¢n c√¥ng: 701 l·ªõp\n",
      "- Ph√¢n c√¥ng ch√©o khoa: 0 l·ªõp\n",
      "- Kh√¥ng th·ªÉ ph√¢n c√¥ng: 0 l·ªõp\n",
      "- T·ªïng l·ªõp c·∫ßn ph√¢n c√¥ng: 701\n",
      "\n",
      "=== TH·ªêNG K√ä WORKLOAD GI·∫¢NG VI√äN ===\n",
      "- S·ªë gi·∫£ng vi√™n ƒë∆∞·ª£c ph√¢n c√¥ng: 242\n",
      "- S·ªë l·ªõp trung b√¨nh/gi·∫£ng vi√™n: 2.9\n",
      "- S·ªë l·ªõp t·ªëi ƒëa/gi·∫£ng vi√™n: 9\n",
      "- S·ªë l·ªõp t·ªëi thi·ªÉu/gi·∫£ng vi√™n: 1\n"
     ]
    }
   ],
   "source": [
    "# === PH√ÇN C√îNG GI·∫¢NG VI√äN TH√îNG MINH (C·∫¢I THI·ªÜN V2) ===\n",
    "print(\"=== PH√ÇN C√îNG GI·∫¢NG VI√äN TH√îNG MINH V2 ===\")\n",
    "\n",
    "def assign_teachers_smart_v2(df_final, df_teachers, df_subjects_clean):\n",
    "    \"\"\"\n",
    "    Thu·∫≠t to√°n ph√¢n c√¥ng gi·∫£ng vi√™n th√¥ng minh V2:\n",
    "    1. ∆Øu ti√™n ph√¢n c√¥ng c√πng khoa\n",
    "    2. Ph√¢n c√¥ng ch√©o khoa khi c·∫ßn thi·∫øt (v·ªõi ki·ªÉm tra t∆∞∆°ng th√≠ch)\n",
    "    3. Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian ch·∫∑t ch·∫Ω\n",
    "    4. T·ªëi ∆∞u h√≥a ph√¢n b·ªï workload\n",
    "    \"\"\"\n",
    "    # Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "    df_result = df_final.copy()\n",
    "    df_result['ma_giang_vien'] = None\n",
    "    \n",
    "    # Track workload c·ªßa gi·∫£ng vi√™n\n",
    "    teacher_workload = {teacher_id: [] for teacher_id in df_teachers['ma_giang_vien']}\n",
    "    \n",
    "    # Danh s√°ch c√°c l·ªõp c·∫ßn ph√¢n c√¥ng (lo·∫°i tr·ª´ nh·ªØng l·ªõp kh√¥ng c·∫ßn)\n",
    "    classes_to_assign = df_result[~df_result['loai_tru']].copy().reset_index()\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo m·ª©c ƒë·ªô ∆∞u ti√™n: c√πng khoa tr∆∞·ªõc, sau ƒë√≥ theo th·ªùi gian\n",
    "    classes_to_assign = classes_to_assign.sort_values(['khoa_bo_mon_quan_ly', 'thu', 'tiet_bat_dau'])\n",
    "    \n",
    "    assigned_count = 0\n",
    "    conflict_count = 0\n",
    "    no_teacher_count = 0\n",
    "    cross_dept_assignments = 0\n",
    "    \n",
    "    print(\"B·∫Øt ƒë·∫ßu ph√¢n c√¥ng theo m·ª©c ƒë·ªô ∆∞u ti√™n...\")\n",
    "    \n",
    "    def check_time_conflict(teacher_schedule, day, start, end):\n",
    "        \"\"\"Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian ch·∫∑t ch·∫Ω\"\"\"\n",
    "        if pd.isna(day) or pd.isna(start) or pd.isna(end):\n",
    "            return False\n",
    "            \n",
    "        for existing_class in teacher_schedule:\n",
    "            if (existing_class['thu'] == day and \n",
    "                not (existing_class['tiet_ket_thuc'] <= start or \n",
    "                     existing_class['tiet_bat_dau'] >= end)):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_compatible_departments(dept):\n",
    "        \"\"\"L·∫•y danh s√°ch khoa t∆∞∆°ng th√≠ch ƒë·ªÉ ph√¢n c√¥ng ch√©o\"\"\"\n",
    "        compatibility_matrix = {\n",
    "            'KTMT': ['KHMT', 'HTTT', 'CNPM'],\n",
    "            'KHMT': ['KTMT', 'HTTT', 'CNPM'],  \n",
    "            'HTTT': ['KHMT', 'KTMT', 'CNPM'],\n",
    "            'KTTT': ['KTMT', 'KHMT', 'HTTT'],\n",
    "            'PƒêTƒêH': ['MMTTT', 'CNPM', 'HTTT'],\n",
    "            'BMTL': ['MMTTT', 'CNPM', 'HTTT'],\n",
    "            'TTNN': ['MMTTT', 'CNPM', 'HTTT'],\n",
    "            'MMTTT': ['CNPM', 'HTTT', 'KTMT']\n",
    "        }\n",
    "        return compatibility_matrix.get(dept, ['CNPM', 'MMTTT', 'HTTT'])\n",
    "    \n",
    "    for idx, class_row in classes_to_assign.iterrows():\n",
    "        original_idx = class_row['index']\n",
    "        class_dept = class_row['khoa_bo_mon_quan_ly']\n",
    "        class_day = class_row['thu']\n",
    "        class_start = class_row['tiet_bat_dau']\n",
    "        class_end = class_row['tiet_ket_thuc']\n",
    "        \n",
    "        # Skip n·∫øu th·ªùi gian kh√¥ng h·ª£p l·ªá\n",
    "        if pd.isna(class_day) or pd.isna(class_start) or pd.isna(class_end):\n",
    "            continue\n",
    "            \n",
    "        assigned = False\n",
    "        \n",
    "        # Phase 1: T√¨m gi·∫£ng vi√™n c√πng khoa\n",
    "        same_dept_teachers = df_teachers[df_teachers['khoa_bo_mon'] == class_dept]\n",
    "        \n",
    "        # S·∫Øp x·∫øp theo workload tƒÉng d·∫ßn ƒë·ªÉ ph√¢n b·ªï ƒë·ªÅu\n",
    "        teacher_workload_list = [(tid, len(teacher_workload[tid])) for tid in same_dept_teachers['ma_giang_vien']]\n",
    "        teacher_workload_list.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for teacher_id, _ in teacher_workload_list:\n",
    "            # Ki·ªÉm tra gi·ªõi h·∫°n workload\n",
    "            if len(teacher_workload[teacher_id]) >= 10:  # Gi·ªõi h·∫°n 10 l·ªõp/gi·∫£ng vi√™n\n",
    "                continue\n",
    "                \n",
    "            # Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian\n",
    "            if not check_time_conflict(teacher_workload[teacher_id], class_day, class_start, class_end):\n",
    "                # Ph√¢n c√¥ng th√†nh c√¥ng\n",
    "                df_result.loc[original_idx, 'ma_giang_vien'] = teacher_id\n",
    "                teacher_workload[teacher_id].append({\n",
    "                    'thu': class_day,\n",
    "                    'tiet_bat_dau': class_start,\n",
    "                    'tiet_ket_thuc': class_end,\n",
    "                    'ma_lop': class_row['ma_lop']\n",
    "                })\n",
    "                assigned = True\n",
    "                assigned_count += 1\n",
    "                break\n",
    "        \n",
    "        # Phase 2: N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c gi·∫£ng vi√™n c√πng khoa, t√¨m gi·∫£ng vi√™n kh√°c khoa\n",
    "        if not assigned:\n",
    "            compatible_depts = get_compatible_departments(class_dept)\n",
    "            \n",
    "            for compatible_dept in compatible_depts:\n",
    "                cross_dept_teachers = df_teachers[df_teachers['khoa_bo_mon'] == compatible_dept]\n",
    "                \n",
    "                # S·∫Øp x·∫øp theo workload\n",
    "                cross_teacher_workload_list = [(tid, len(teacher_workload[tid])) for tid in cross_dept_teachers['ma_giang_vien']]\n",
    "                cross_teacher_workload_list.sort(key=lambda x: x[1])\n",
    "                \n",
    "                for teacher_id, current_workload in cross_teacher_workload_list:\n",
    "                    # ∆Øu ti√™n gi·∫£ng vi√™n c√≥ √≠t l·ªõp h∆°n v√† gi·ªõi h·∫°n workload ch√©o khoa\n",
    "                    if current_workload >= 8:  # Gi·ªõi h·∫°n th·∫•p h∆°n cho ph√¢n c√¥ng ch√©o\n",
    "                        continue\n",
    "                    \n",
    "                    # Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian\n",
    "                    if not check_time_conflict(teacher_workload[teacher_id], class_day, class_start, class_end):\n",
    "                        # Ph√¢n c√¥ng ch√©o khoa\n",
    "                        df_result.loc[original_idx, 'ma_giang_vien'] = teacher_id\n",
    "                        teacher_workload[teacher_id].append({\n",
    "                            'thu': class_day,\n",
    "                            'tiet_bat_dau': class_start,\n",
    "                            'tiet_ket_thuc': class_end,\n",
    "                            'ma_lop': class_row['ma_lop']\n",
    "                        })\n",
    "                        assigned = True\n",
    "                        assigned_count += 1\n",
    "                        cross_dept_assignments += 1\n",
    "                        break\n",
    "                \n",
    "                if assigned:\n",
    "                    break\n",
    "        \n",
    "        if not assigned:\n",
    "            no_teacher_count += 1\n",
    "    \n",
    "    print(f\"\\nK·∫øt qu·∫£ ph√¢n c√¥ng th√¥ng minh V2:\")\n",
    "    print(f\"- ƒê√£ ph√¢n c√¥ng: {assigned_count} l·ªõp\")\n",
    "    print(f\"- Ph√¢n c√¥ng ch√©o khoa: {cross_dept_assignments} l·ªõp\")\n",
    "    print(f\"- Kh√¥ng th·ªÉ ph√¢n c√¥ng: {no_teacher_count} l·ªõp\")\n",
    "    print(f\"- T·ªïng l·ªõp c·∫ßn ph√¢n c√¥ng: {len(classes_to_assign)}\")\n",
    "    \n",
    "    # Th·ªëng k√™ workload\n",
    "    teacher_counts = [len(workload) for workload in teacher_workload.values() if len(workload) > 0]\n",
    "    if teacher_counts:\n",
    "        print(f\"\\n=== TH·ªêNG K√ä WORKLOAD GI·∫¢NG VI√äN ===\")\n",
    "        print(f\"- S·ªë gi·∫£ng vi√™n ƒë∆∞·ª£c ph√¢n c√¥ng: {len(teacher_counts)}\")\n",
    "        print(f\"- S·ªë l·ªõp trung b√¨nh/gi·∫£ng vi√™n: {np.mean(teacher_counts):.1f}\")\n",
    "        print(f\"- S·ªë l·ªõp t·ªëi ƒëa/gi·∫£ng vi√™n: {max(teacher_counts)}\")\n",
    "        print(f\"- S·ªë l·ªõp t·ªëi thi·ªÉu/gi·∫£ng vi√™n: {min(teacher_counts)}\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# √Åp d·ª•ng thu·∫≠t to√°n m·ªõi V2\n",
    "df_final = assign_teachers_smart_v2(df_final, df_teachers, df_subjects_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c3fbc",
   "metadata": {},
   "source": [
    "## 5. Validate Assignment Constraints\n",
    "\n",
    "Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa vi·ªác ph√¢n c√¥ng gi·∫£ng vi√™n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c8670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ ===\n",
      "1. Ki·ªÉm tra khoa/b·ªô m√¥n ph√π h·ª£p...\n",
      "2. Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian...\n",
      "3. Ki·ªÉm tra c√°c m√¥n lo·∫°i tr·ª´...\n",
      "\n",
      "K·∫øt qu·∫£ ki·ªÉm tra:\n",
      "- L·ªói khoa/b·ªô m√¥n kh√¥ng ph√π h·ª£p: 0\n",
      "- L·ªói xung ƒë·ªôt th·ªùi gian: 0\n",
      "- L·ªói ph√¢n c√¥ng m√¥n lo·∫°i tr·ª´: 0\n",
      "\n",
      "Ki·ªÉm tra ho√†n t·∫•t!\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa ph√¢n c√¥ng\n",
    "print(\"=== KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ ===\")\n",
    "\n",
    "def validate_assignments(df_final, df_teachers):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra c√°c r√†ng bu·ªôc c·ªßa vi·ªác ph√¢n c√¥ng gi·∫£ng vi√™n\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'department_mismatch': [],\n",
    "        'time_conflicts': [],\n",
    "        'invalid_exclusions': []\n",
    "    }\n",
    "    \n",
    "    # 1. Ki·ªÉm tra khoa/b·ªô m√¥n ph√π h·ª£p\n",
    "    print(\"1. Ki·ªÉm tra khoa/b·ªô m√¥n ph√π h·ª£p...\")\n",
    "    assigned_classes = df_final[~df_final['loai_tru'] & df_final['ma_giang_vien'].notna()]\n",
    "    \n",
    "    for idx, row in assigned_classes.iterrows():\n",
    "        teacher_id = row['ma_giang_vien']\n",
    "        subject_dept = row['khoa_bo_mon_quan_ly']\n",
    "        \n",
    "        if pd.notna(teacher_id):\n",
    "            teacher_info = df_teachers[df_teachers['ma_giang_vien'] == teacher_id]\n",
    "            if len(teacher_info) > 0:\n",
    "                teacher_dept = teacher_info.iloc[0]['khoa_bo_mon']\n",
    "                if teacher_dept != subject_dept:\n",
    "                    validation_results['department_mismatch'].append({\n",
    "                        'ma_lop': row['ma_lop'],\n",
    "                        'ma_giang_vien': teacher_id,\n",
    "                        'teacher_dept': teacher_dept,\n",
    "                        'subject_dept': subject_dept\n",
    "                    })\n",
    "    \n",
    "    # 2. Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian\n",
    "    print(\"2. Ki·ªÉm tra xung ƒë·ªôt th·ªùi gian...\")\n",
    "    teacher_schedules = {}\n",
    "    \n",
    "    for idx, row in assigned_classes.iterrows():\n",
    "        teacher_id = row['ma_giang_vien']\n",
    "        if pd.isna(teacher_id) or pd.isna(row['tiet_bat_dau']):\n",
    "            continue\n",
    "            \n",
    "        schedule_key = f\"{row['thu']}_{row['tiet_bat_dau']}_{row['tiet_ket_thuc']}\"\n",
    "        \n",
    "        if teacher_id not in teacher_schedules:\n",
    "            teacher_schedules[teacher_id] = []\n",
    "        \n",
    "        # Ki·ªÉm tra xung ƒë·ªôt v·ªõi l·ªãch hi·ªán t·∫°i\n",
    "        for existing in teacher_schedules[teacher_id]:\n",
    "            if (existing['thu'] == row['thu'] and \n",
    "                not (row['tiet_ket_thuc'] < existing['tiet_start'] or \n",
    "                     row['tiet_bat_dau'] > existing['tiet_end'])):\n",
    "                validation_results['time_conflicts'].append({\n",
    "                    'ma_giang_vien': teacher_id,\n",
    "                    'conflict_classes': [existing['ma_lop'], row['ma_lop']],\n",
    "                    'thu': row['thu'],\n",
    "                    'time_overlap': f\"{max(row['tiet_bat_dau'], existing['tiet_start'])}-{min(row['tiet_ket_thuc'], existing['tiet_end'])}\"\n",
    "                })\n",
    "        \n",
    "        teacher_schedules[teacher_id].append({\n",
    "            'ma_lop': row['ma_lop'],\n",
    "            'thu': row['thu'],\n",
    "            'tiet_start': row['tiet_bat_dau'],\n",
    "            'tiet_end': row['tiet_ket_thuc']\n",
    "        })\n",
    "    \n",
    "    # 3. Ki·ªÉm tra c√°c m√¥n b·ªã lo·∫°i tr·ª´ kh√¥ng ƒë∆∞·ª£c ph√¢n c√¥ng\n",
    "    print(\"3. Ki·ªÉm tra c√°c m√¥n lo·∫°i tr·ª´...\")\n",
    "    excluded_with_teachers = df_final[df_final['loai_tru'] & df_final['ma_giang_vien'].notna()]\n",
    "    for idx, row in excluded_with_teachers.iterrows():\n",
    "        validation_results['invalid_exclusions'].append({\n",
    "            'ma_lop': row['ma_lop'],\n",
    "            'ma_mon_hoc': row['ma_mon_hoc'],\n",
    "            'hinh_thuc': row['hinh_thuc_giang_day'],\n",
    "            'ma_giang_vien': row['ma_giang_vien']\n",
    "        })\n",
    "    \n",
    "    # In k·∫øt qu·∫£ ki·ªÉm tra\n",
    "    print(f\"\\nK·∫øt qu·∫£ ki·ªÉm tra:\")\n",
    "    print(f\"- L·ªói khoa/b·ªô m√¥n kh√¥ng ph√π h·ª£p: {len(validation_results['department_mismatch'])}\")\n",
    "    print(f\"- L·ªói xung ƒë·ªôt th·ªùi gian: {len(validation_results['time_conflicts'])}\")\n",
    "    print(f\"- L·ªói ph√¢n c√¥ng m√¥n lo·∫°i tr·ª´: {len(validation_results['invalid_exclusions'])}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Th·ª±c hi·ªán ki·ªÉm tra\n",
    "validation_results = validate_assignments(df_final, df_teachers)\n",
    "\n",
    "# Hi·ªÉn th·ªã chi ti·∫øt l·ªói n·∫øu c√≥\n",
    "if validation_results['department_mismatch']:\n",
    "    print(\"\\nChi ti·∫øt l·ªói khoa/b·ªô m√¥n kh√¥ng ph√π h·ª£p:\")\n",
    "    for error in validation_results['department_mismatch'][:5]:  # Hi·ªÉn th·ªã 5 l·ªói ƒë·∫ßu\n",
    "        print(f\"  - L·ªõp {error['ma_lop']}: GV {error['ma_giang_vien']} ({error['teacher_dept']}) d·∫°y m√¥n thu·ªôc {error['subject_dept']}\")\n",
    "\n",
    "if validation_results['time_conflicts']:\n",
    "    print(\"\\nChi ti·∫øt xung ƒë·ªôt th·ªùi gian:\")\n",
    "    for error in validation_results['time_conflicts'][:5]:  # Hi·ªÉn th·ªã 5 l·ªói ƒë·∫ßu\n",
    "        print(f\"  - GV {error['ma_giang_vien']} c√≥ xung ƒë·ªôt th·ª© {error['thu']}: {error['conflict_classes']}\")\n",
    "\n",
    "print(\"\\nKi·ªÉm tra ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef0724",
   "metadata": {},
   "source": [
    "## 6. Export Normalized Data\n",
    "\n",
    "Xu·∫•t d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a ra file CSV ƒë·ªÉ import v√†o database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82289ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHU·∫®N B·ªä D·ªÆ LI·ªÜU EXPORT ===\n",
      "=== TH·ªêNG K√ä T·ªîNG K·∫æT ===\n",
      "T·ªïng s·ªë l·ªõp: 824\n",
      "S·ªë l·ªõp ƒë√£ ƒë∆∞·ª£c ph√¢n c√¥ng gi·∫£ng vi√™n: 701\n",
      "S·ªë l·ªõp kh√¥ng c·∫ßn ph√¢n c√¥ng (m√£ GV = '*'): 123\n",
      "S·ªë l·ªõp ch∆∞a c√≥ gi·∫£ng vi√™n (tr·ªëng): 0\n",
      "\n",
      "Ph√¢n b·ªë theo h√¨nh th·ª©c gi·∫£ng d·∫°y:\n",
      "hinh_thuc_giang_day\n",
      "LT      414\n",
      "HT1     296\n",
      "HT2      82\n",
      "ƒêA       12\n",
      "KLTN     10\n",
      "TTTN     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PH√ÇN T√çCH CHI TI·∫æT ===\n",
      "L·ªõp c√≥ m√£ gi·∫£ng vi√™n '*' (kh√¥ng c·∫ßn ph√¢n c√¥ng):\n",
      "Ph√¢n b·ªë theo h√¨nh th·ª©c:\n",
      "hinh_thuc_giang_day\n",
      "HT2     82\n",
      "ƒêA      12\n",
      "TTTN    10\n",
      "KLTN    10\n",
      "LT       9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "M·ªôt s·ªë v√≠ d·ª•:\n",
      "ma_mon_hoc    ma_lop hinh_thuc_giang_day thu                                                ghi_chu\n",
      "     PE231 PE231.P25                  LT   7 B∆°i l·ªôi, h·ªçc t·∫°i https://goo.gl/maps/NGD24VkSEtJmJ5pJ6\n",
      "     CE505 CE505.P21                KLTN   *                                                    NaN\n",
      "     CE502 CE502.P21                TTTN   *                                                    NaN\n",
      "     CE201 CE201.P21                  ƒêA   *                                                    NaN\n",
      "     CE206 CE206.P21                  ƒêA   *                                                    NaN\n",
      "     CE408 CE408.P21                  ƒêA   *                                              m√¥n ƒë·ªì √°n\n",
      "     CE412 CE412.P21                  ƒêA   *                                              m√¥n ƒë·ªì √°n\n",
      "     IE505 IE505.P21                KLTN   *                                                    NaN\n",
      "     DS505 DS505.P21                KLTN   *                                                    NaN\n",
      "     IE207 IE207.P21                  ƒêA   *                                                    NaN\n",
      "\n",
      "ƒê√£ xu·∫•t d·ªØ li·ªáu ra file: d:\\eUIT\\scripts\\database\\main_data\\thoi_khoa_bieu_normalized.csv\n",
      "ƒê√£ t·∫°o b√°o c√°o chi ti·∫øt: d:\\eUIT\\scripts\\database\\main_data\\assignment_report.json\n",
      "\n",
      "=== HO√ÄN T·∫§T CHU·∫®N H√ìA D·ªÆ LI·ªÜU ===\n",
      "\n",
      "=== T√ìM T·∫ÆT K·∫æT QU·∫¢ ===\n",
      "- L·ªõp ƒë√£ ph√¢n c√¥ng GV: 701\n",
      "- L·ªõp kh√¥ng c·∫ßn GV (m√£ '*'): 123\n",
      "- L·ªõp ch∆∞a c√≥ GV: 0\n",
      "- T·ªïng c·ªông: 824 l·ªõp\n"
     ]
    }
   ],
   "source": [
    "# Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ export\n",
    "print(\"=== CHU·∫®N B·ªä D·ªÆ LI·ªÜU EXPORT ===\")\n",
    "\n",
    "# T·∫°o DataFrame cu·ªëi c√πng v·ªõi c·∫•u tr√∫c ph√π h·ª£p v·ªõi database\n",
    "df_export = df_final.copy()\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng ng√†y v·ªÅ chu·ªói\n",
    "df_export['ngay_bat_dau'] = df_export['ngay_bat_dau'].dt.strftime('%Y-%m-%d')\n",
    "df_export['ngay_ket_thuc'] = df_export['ngay_ket_thuc'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# X·ª≠ l√Ω m√£ gi·∫£ng vi√™n:\n",
    "# - C√°c l·ªõp ƒë√£ ƒë∆∞·ª£c ph√¢n c√¥ng: gi·ªØ nguy√™n m√£ gi·∫£ng vi√™n\n",
    "# - C√°c l·ªõp lo·∫°i tr·ª´ (kh√¥ng c·∫ßn ph√¢n c√¥ng): g√°n m√£ gi·∫£ng vi√™n = \"*\"\n",
    "# - C√°c l·ªõp kh√°c (n·∫øu c√≥): ƒë·ªÉ tr·ªëng\n",
    "df_export.loc[df_export['loai_tru'], 'ma_giang_vien'] = '*'\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi c√°c gi√° tr·ªã null c√≤n l·∫°i v·ªÅ integer r·ªìi v·ªÅ string\n",
    "df_export['ma_giang_vien'] = df_export['ma_giang_vien'].fillna(0)\n",
    "# Chuy·ªÉn c√°c gi√° tr·ªã s·ªë v·ªÅ integer (tr·ª´ '*')\n",
    "mask_numeric = df_export['ma_giang_vien'] != '*'\n",
    "df_export.loc[mask_numeric, 'ma_giang_vien'] = df_export.loc[mask_numeric, 'ma_giang_vien'].astype(int)\n",
    "# Chuy·ªÉn 0 th√†nh chu·ªói r·ªóng (n·∫øu c√≥ l·ªõp n√†o kh√¥ng ƒë∆∞·ª£c ph√¢n c√¥ng v√† kh√¥ng b·ªã lo·∫°i tr·ª´)\n",
    "df_export['ma_giang_vien'] = df_export['ma_giang_vien'].replace(0, '')\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi tiet_bat_dau v√† tiet_ket_thuc v·ªÅ integer (n·∫øu kh√¥ng ph·∫£i NaN)\n",
    "df_export['tiet_bat_dau'] = df_export['tiet_bat_dau'].fillna(0).astype(int)\n",
    "df_export['tiet_bat_dau'] = df_export['tiet_bat_dau'].replace(0, '')\n",
    "\n",
    "df_export['tiet_ket_thuc'] = df_export['tiet_ket_thuc'].fillna(0).astype(int)\n",
    "df_export['tiet_ket_thuc'] = df_export['tiet_ket_thuc'].replace(0, '')\n",
    "\n",
    "# Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt theo c·∫•u tr√∫c database\n",
    "columns_to_export = [\n",
    "    'hoc_ky', 'ma_mon_hoc', 'ma_lop', 'so_tin_chi', 'ma_giang_vien',\n",
    "    'thu', 'tiet_bat_dau', 'tiet_ket_thuc', 'cach_tuan', \n",
    "    'ngay_bat_dau', 'ngay_ket_thuc', 'phong_hoc', 'si_so', \n",
    "    'hinh_thuc_giang_day', 'ghi_chu'\n",
    "]\n",
    "\n",
    "df_export_final = df_export[columns_to_export].copy()\n",
    "\n",
    "# T·∫°o th·ªëng k√™ t·ªïng k·∫øt\n",
    "print(\"=== TH·ªêNG K√ä T·ªîNG K·∫æT ===\")\n",
    "print(f\"T·ªïng s·ªë l·ªõp: {len(df_export_final)}\")\n",
    "assigned_count = (df_export_final['ma_giang_vien'] != '') & (df_export_final['ma_giang_vien'] != '*')\n",
    "excluded_count = df_export_final['ma_giang_vien'] == '*'\n",
    "unassigned_count = df_export_final['ma_giang_vien'] == ''\n",
    "\n",
    "print(f\"S·ªë l·ªõp ƒë√£ ƒë∆∞·ª£c ph√¢n c√¥ng gi·∫£ng vi√™n: {assigned_count.sum()}\")\n",
    "print(f\"S·ªë l·ªõp kh√¥ng c·∫ßn ph√¢n c√¥ng (m√£ GV = '*'): {excluded_count.sum()}\")\n",
    "print(f\"S·ªë l·ªõp ch∆∞a c√≥ gi·∫£ng vi√™n (tr·ªëng): {unassigned_count.sum()}\")\n",
    "\n",
    "print(\"\\nPh√¢n b·ªë theo h√¨nh th·ª©c gi·∫£ng d·∫°y:\")\n",
    "print(df_export_final['hinh_thuc_giang_day'].value_counts())\n",
    "\n",
    "# Ph√¢n t√≠ch chi ti·∫øt c√°c lo·∫°i l·ªõp\n",
    "print(f\"\\n=== PH√ÇN T√çCH CHI TI·∫æT ===\")\n",
    "print(\"L·ªõp c√≥ m√£ gi·∫£ng vi√™n '*' (kh√¥ng c·∫ßn ph√¢n c√¥ng):\")\n",
    "excluded_classes = df_export_final[df_export_final['ma_giang_vien'] == '*']\n",
    "if len(excluded_classes) > 0:\n",
    "    print(\"Ph√¢n b·ªë theo h√¨nh th·ª©c:\")\n",
    "    print(excluded_classes['hinh_thuc_giang_day'].value_counts())\n",
    "    \n",
    "    print(\"\\nM·ªôt s·ªë v√≠ d·ª•:\")\n",
    "    sample_excluded = excluded_classes[['ma_mon_hoc', 'ma_lop', 'hinh_thuc_giang_day', 'thu', 'ghi_chu']].head(10)\n",
    "    print(sample_excluded.to_string(index=False))\n",
    "\n",
    "# Export ra file CSV\n",
    "output_path = r\"d:\\eUIT\\scripts\\database\\main_data\\thoi_khoa_bieu_normalized.csv\"\n",
    "df_export_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nƒê√£ xu·∫•t d·ªØ li·ªáu ra file: {output_path}\")\n",
    "\n",
    "# T·∫°o file b√°o c√°o chi ti·∫øt - S·ª¨A L·ªñI JSON SERIALIZATION\n",
    "report_data = {\n",
    "    'total_classes': int(len(df_export_final)),\n",
    "    'assigned_classes': int(assigned_count.sum()),\n",
    "    'excluded_classes': int(excluded_count.sum()),\n",
    "    'unassigned_classes': int(unassigned_count.sum()),\n",
    "    'validation_errors': {\n",
    "        'department_mismatch': len(validation_results['department_mismatch']),\n",
    "        'time_conflicts': len(validation_results['time_conflicts']),\n",
    "        'invalid_exclusions': len(validation_results['invalid_exclusions'])\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "report_path = r\"d:\\eUIT\\scripts\\database\\main_data\\assignment_report.json\"\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ƒê√£ t·∫°o b√°o c√°o chi ti·∫øt: {report_path}\")\n",
    "print(\"\\n=== HO√ÄN T·∫§T CHU·∫®N H√ìA D·ªÆ LI·ªÜU ===\")\n",
    "\n",
    "print(\"\\n=== T√ìM T·∫ÆT K·∫æT QU·∫¢ ===\")\n",
    "print(f\"- L·ªõp ƒë√£ ph√¢n c√¥ng GV: {assigned_count.sum()}\")\n",
    "print(f\"- L·ªõp kh√¥ng c·∫ßn GV (m√£ '*'): {excluded_count.sum()}\")\n",
    "print(f\"- L·ªõp ch∆∞a c√≥ GV: {unassigned_count.sum()}\")\n",
    "print(f\"- T·ªïng c·ªông: {len(df_export_final)} l·ªõp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dad33",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt\n",
    "\n",
    "Notebook n√†y ƒë√£ th·ª±c hi·ªán th√†nh c√¥ng vi·ªác chu·∫©n h√≥a d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu v√† ph√¢n c√¥ng gi·∫£ng vi√™n v·ªõi c√°c quy t·∫Øc sau:\n",
    "\n",
    "### ‚úÖ ƒê√£ th·ª±c hi·ªán:\n",
    "1. **Chu·∫©n h√≥a d·ªØ li·ªáu**: L√†m s·∫°ch v√† ƒë·ªãnh d·∫°ng l·∫°i d·ªØ li·ªáu th·ªùi kh√≥a bi·ªÉu\n",
    "2. **Ph√¢n c√¥ng gi·∫£ng vi√™n**: D·ª±a tr√™n khoa/b·ªô m√¥n ph√π h·ª£p\n",
    "3. **Ki·ªÉm tra xung ƒë·ªôt**: ƒê·∫£m b·∫£o gi·∫£ng vi√™n kh√¥ng d·∫°y 2 l·ªõp c√πng l√∫c\n",
    "4. **Lo·∫°i tr·ª´ m√¥n h·ªçc**: DA, TTTN, KLTN, Ti·∫øng Nh·∫≠t (NT), PE231\n",
    "5. **Gi·ªØ nguy√™n ph√≤ng h·ªçc**: C√°c ph√≤ng c√≥ d·∫•u \"*\" (online/kh√¥ng ph√¢n chia)\n",
    "\n",
    "### üìä K·∫øt qu·∫£:\n",
    "- D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c xu·∫•t ra file: `thoi_khoa_bieu_normalized.csv`\n",
    "- B√°o c√°o chi ti·∫øt: `assignment_report.json`\n",
    "- T·∫•t c·∫£ r√†ng bu·ªôc ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† validate\n",
    "\n",
    "### üöÄ B∆∞·ªõc ti·∫øp theo:\n",
    "- Import d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a v√†o PostgreSQL database\n",
    "- S·ª≠ d·ª•ng c√°c script SQL ƒë·ªÉ t·∫°o b·∫£ng v√† import d·ªØ li·ªáu\n",
    "- Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu trong database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1302c9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PH√ÇN T√çCH CHI TI·∫æT V·∫§N ƒê·ªÄ PH√ÇN C√îNG ===\n",
      "1. S·ªë l∆∞·ª£ng gi·∫£ng vi√™n theo khoa:\n",
      "khoa_bo_mon\n",
      "HTTT     55\n",
      "KHMT     45\n",
      "KTMT     35\n",
      "MMTTT    32\n",
      "KTTT     27\n",
      "CNPM     23\n",
      "PƒêTƒêH    13\n",
      "BMTL      6\n",
      "TTNN      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. S·ªë l∆∞·ª£ng l·ªõp c·∫ßn ph√¢n c√¥ng theo khoa:\n",
      "khoa_bo_mon_quan_ly\n",
      "KTMT     115\n",
      "KHMT      91\n",
      "HTTT      86\n",
      "KTTT      59\n",
      "PƒêTƒêH     58\n",
      "BMTL      50\n",
      "TTNN      26\n",
      "MMTTT     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. T·ª∑ l·ªá gi·∫£ng vi√™n/l·ªõp theo khoa:\n",
      "       giang_vien  lop_can_phan_cong     ty_le  lop_per_gv\n",
      "BMTL            6               50.0  0.120000    8.333333\n",
      "CNPM           23                0.0       inf    0.000000\n",
      "HTTT           55               86.0  0.639535    1.563636\n",
      "KHMT           45               91.0  0.494505    2.022222\n",
      "KTMT           35              115.0  0.304348    3.285714\n",
      "KTTT           27               59.0  0.457627    2.185185\n",
      "MMTTT          32               15.0  2.133333    0.468750\n",
      "PƒêTƒêH          13               58.0  0.224138    4.461538\n",
      "TTNN            6               26.0  0.230769    4.333333\n",
      "\n",
      "4. Ph√¢n t√≠ch chi ti·∫øt l·ªõp ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng:\n",
      "T·ªïng l·ªõp ch∆∞a ph√¢n c√¥ng: 94\n",
      "\n",
      "Ph√¢n b·ªë theo khoa:\n",
      "khoa_bo_mon_quan_ly\n",
      "TTNN    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. Ph√¢n t√≠ch th·ªùi gian c·ªßa l·ªõp ch∆∞a ph√¢n c√¥ng:\n",
      "Top 10 slot th·ªùi gian c√≥ nhi·ªÅu l·ªõp ch∆∞a ph√¢n c√¥ng:\n",
      "  khoa_bo_mon_quan_ly thu  tiet_bat_dau  count\n",
      "0                TTNN   5           0.0      1\n",
      "\n",
      "6. Gi·∫£ng vi√™n ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng:\n",
      "S·ªë gi·∫£ng vi√™n ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng: 40\n",
      "Ph√¢n b·ªë theo khoa:\n",
      "khoa_bo_mon\n",
      "CNPM     23\n",
      "MMTTT    17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === PH√ÇN T√çCH PROBLEM V·ªöI THU·∫¨T TO√ÅN ===\n",
    "print(\"=== PH√ÇN T√çCH CHI TI·∫æT V·∫§N ƒê·ªÄ PH√ÇN C√îNG ===\")\n",
    "\n",
    "# 1. Ph√¢n t√≠ch s·ªë l∆∞·ª£ng gi·∫£ng vi√™n theo khoa\n",
    "print(\"1. S·ªë l∆∞·ª£ng gi·∫£ng vi√™n theo khoa:\")\n",
    "teacher_by_dept = df_teachers['khoa_bo_mon'].value_counts()\n",
    "print(teacher_by_dept)\n",
    "\n",
    "# 2. Ph√¢n t√≠ch s·ªë l∆∞·ª£ng l·ªõp c·∫ßn ph√¢n c√¥ng theo khoa\n",
    "print(\"\\n2. S·ªë l∆∞·ª£ng l·ªõp c·∫ßn ph√¢n c√¥ng theo khoa:\")\n",
    "classes_need_assign = df_final[~df_final['loai_tru']]['khoa_bo_mon_quan_ly'].value_counts()\n",
    "print(classes_need_assign)\n",
    "\n",
    "# 3. So s√°nh t·ª∑ l·ªá gi·∫£ng vi√™n/l·ªõp theo khoa\n",
    "print(\"\\n3. T·ª∑ l·ªá gi·∫£ng vi√™n/l·ªõp theo khoa:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'giang_vien': teacher_by_dept,\n",
    "    'lop_can_phan_cong': classes_need_assign,\n",
    "}).fillna(0)\n",
    "comparison['ty_le'] = comparison['giang_vien'] / comparison['lop_can_phan_cong']\n",
    "comparison['lop_per_gv'] = comparison['lop_can_phan_cong'] / comparison['giang_vien']\n",
    "print(comparison)\n",
    "\n",
    "# 4. Ph√¢n t√≠ch c√°c l·ªõp ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng\n",
    "print(\"\\n4. Ph√¢n t√≠ch chi ti·∫øt l·ªõp ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng:\")\n",
    "unassigned_analysis = df_final[\n",
    "    (~df_final['loai_tru']) & \n",
    "    (df_final['ma_giang_vien'].isna())\n",
    "].copy()\n",
    "\n",
    "print(f\"T·ªïng l·ªõp ch∆∞a ph√¢n c√¥ng: {len(unassigned_analysis)}\")\n",
    "print(\"\\nPh√¢n b·ªë theo khoa:\")\n",
    "unassigned_by_dept = unassigned_analysis['khoa_bo_mon_quan_ly'].value_counts()\n",
    "print(unassigned_by_dept)\n",
    "\n",
    "# 5. Ph√¢n t√≠ch th·ªùi gian c·ªßa c√°c l·ªõp ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng\n",
    "print(\"\\n5. Ph√¢n t√≠ch th·ªùi gian c·ªßa l·ªõp ch∆∞a ph√¢n c√¥ng:\")\n",
    "time_analysis = unassigned_analysis.groupby(['khoa_bo_mon_quan_ly', 'thu', 'tiet_bat_dau']).size().reset_index(name='count')\n",
    "print(\"Top 10 slot th·ªùi gian c√≥ nhi·ªÅu l·ªõp ch∆∞a ph√¢n c√¥ng:\")\n",
    "print(time_analysis.nlargest(10, 'count'))\n",
    "\n",
    "# 6. Ki·ªÉm tra c√≥ gi·∫£ng vi√™n n√†o ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng l·ªõp n√†o kh√¥ng\n",
    "print(\"\\n6. Gi·∫£ng vi√™n ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng:\")\n",
    "assigned_teachers = df_final[df_final['ma_giang_vien'].notna()]['ma_giang_vien'].unique()\n",
    "unassigned_teachers = df_teachers[~df_teachers['ma_giang_vien'].isin(assigned_teachers)]\n",
    "print(f\"S·ªë gi·∫£ng vi√™n ch∆∞a ƒë∆∞·ª£c ph√¢n c√¥ng: {len(unassigned_teachers)}\")\n",
    "print(\"Ph√¢n b·ªë theo khoa:\")\n",
    "print(unassigned_teachers['khoa_bo_mon'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
